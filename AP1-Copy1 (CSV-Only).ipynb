{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5308c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task: Required Installations \n",
    "\n",
    "# !pip3 uninstall owlready2 \n",
    "# !pip3 install -U sentence-transformers\n",
    "# !pip3 install neo4j-driver\n",
    "# !pip3 install simpledbf\n",
    "# !pip3 install pysimplegui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6beea905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTables is not installed. No support for HDF output.\n"
     ]
    }
   ],
   "source": [
    "#Task: Import Required Libraries\n",
    "\n",
    "import re\n",
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import pprint\n",
    "import types\n",
    "from neo4j import GraphDatabase\n",
    "from simpledbf import Dbf5\n",
    "import PySimpleGUI as sg\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee9d75",
   "metadata": {},
   "source": [
    "## Load Windows (File and Ontology Specification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d52ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create GUI: argument of type 'NoneType' is not iterable\n"
     ]
    }
   ],
   "source": [
    "#Task: Upload Structured Data: CSV or SQL File and convert into Dataset\n",
    "#Reference: https://www.pysimplegui.org/en/latest/\n",
    "try:\n",
    "    sg.theme('DarkAmber')  \n",
    "    filepath = [sg.FileBrowse()]\n",
    "    uri = [sg.InputText()]\n",
    "    layout = [  [sg.Text('Ontology URI'), uri],\n",
    "                [sg.Text('Data Source file path'), filepath],\n",
    "                [sg.Button('Save')] ]\n",
    "\n",
    "    #Task: Create the Window\n",
    "    window = sg.Window('Ontology Mapping', layout)\n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "        if event == sg.WIN_CLOSED or event == 'Save': \n",
    "            break\n",
    "\n",
    "    window.close()\n",
    "\n",
    "    #Task: Upload Structured Data: CSV or SQL File and convert into Dataset\n",
    "    filename = values[\"Browse\"]\n",
    "    df = pd.read_csv(filename, sep=r\";\", engine=\"python\") if \"csv\" in filename else pd.read_sql(filename) \n",
    "\n",
    "    #Task: Load ontology\n",
    "    onto = get_ontology(values[0])\n",
    "    onto.load()\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Failed to create GUI:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e1f61",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f26c08e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingCSV():\n",
    "    #Task: Preprocessing the uploaded file. Clean up Column Names\n",
    "    keywordsList =[]\n",
    "\n",
    "    #Check if unique column exists. If not, create one\n",
    "    if df.iloc[:,0].is_unique == False:\n",
    "        tempstring = df.columns[0]+\" \"+df.columns[1]\n",
    "        df[str(tempstring)] = df.iloc[:,0].astype(str) +\" \"+  df.iloc[:,1].astype(str) \n",
    "        first_col = df.pop(tempstring)\n",
    "        df.insert(0, str(tempstring), first_col)\n",
    "        df = df.drop([str(df.columns[1]),str(df.columns[2])], axis=1)\n",
    "        for i in range(len(df)):\n",
    "            df[str(tempstring)][i] = str(i)+\" \"+df[str(tempstring)][i]\n",
    "\n",
    "    keywordsList = [re.sub(\"[:,.;'><-=]*\",\"\",clUpperCase) for clUpperCase in df.columns]\n",
    "    df.columns = keywordsList\n",
    "    tempstring = df.columns[0]\n",
    "\n",
    "    return keywordsList\n",
    "\n",
    "def removespaces(var):\n",
    "    var.replace(\" \",\"\")\n",
    "    return var\n",
    "\n",
    "def fetchOntologyClasses():\n",
    "    #Task: Selects All Classes and human readable labels in ontology\n",
    "    classes = list(onto.classes())\n",
    "    print(classes)\n",
    "    newClasses = []\n",
    "    for i in range(len(classes)):newClasses.append(str(classes[i]))\n",
    "\n",
    "    return newClasses\n",
    "\n",
    "def SBert(keywordsList):\n",
    "    \n",
    "    newClasses = fetchOntologyClasses()\n",
    "    \n",
    "    #Task: Use S-Bert to match keywords to class names\n",
    "    #Reference: https://www.sbert.net/docs/usage/semantic_textual_similarity.html\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2') #preTrained model\n",
    "\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(keywordsList, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(newClasses, convert_to_tensor=True)\n",
    "\n",
    "    #Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    #Task: Pick Top Scoring label that matches with keyword as per the cos_sim function, set a threshold and create new class if threshold is not met. \n",
    "    result = dict() #Dictionary with Result for SBert\n",
    "    createClass = dict()\n",
    "    choices = []\n",
    "\n",
    "    k = torch.topk(cosine_scores, 5, dim=1, largest=True, sorted=True)\n",
    "    for i in range(len(keywordsList)):\n",
    "        for index in range(5):\n",
    "    #         print(\"Key Word:{} \\tSuggested Class(label):{} \\tScore:{:.4f}\".format(keywordsList[i], newClasses[k.indices[i][index]], k.values[i][index]))    \n",
    "            createClass.update({keywordsList[i]:keywordsList[i].replace(\" \",\"\")}) if k.values[i][0] < 0.4 else choices.append([keywordsList[i], newClasses[k.indices[i][index]], k.values[i][index]])\n",
    "\n",
    "    return result, createClass, choices\n",
    "\n",
    "def rightClassMapping(choices):\n",
    "    tempdictionary = {}\n",
    "    layout=[]\n",
    "    if any(v is not None for v in choices):\n",
    "        for i in range(len(choices)):\n",
    "            tempkeyword = choices[i][0]\n",
    "            if tempkeyword in tempdictionary:\n",
    "                tempdictionary[tempkeyword].append(choices[i][1])\n",
    "            else:\n",
    "                tempdictionary[tempkeyword] = [choices[i][1]]\n",
    "\n",
    "        layout.append([sg.Text('Confirm if the matched Keyword-Class pair is right',font=(12))])\n",
    "\n",
    "        for i in tempdictionary:\n",
    "            layout.append([[sg.Text(i, p=(0, 0 or (5, 0)))],\n",
    "                          [sg.Combo([tempdictionary[i][0],tempdictionary[i][1],tempdictionary[i][2],\n",
    "                                    tempdictionary[i][3],tempdictionary[i][4]], default_value=tempdictionary[i][0],key=i,\n",
    "                                    size=(20,20))]])\n",
    "\n",
    "        if any(v is not None for v in createClass.keys()):\n",
    "            layout.append([sg.Text('New classes made for keyword/class pairings:',font=(12),p=(0, 0 or (5, 5)))])\n",
    "            for newval in createClass:\n",
    "                layout.append([sg.Text(newval)])\n",
    "\n",
    "        layout.append([sg.Button('SAVE', font=(12))])\n",
    "\n",
    "        #Define Window\n",
    "        win =sg.Window('Customise your Journey',layout)\n",
    "        e,value=win.read()\n",
    "        win.close()\n",
    "\n",
    "    else:\n",
    "        layout.append([sg.Text('New classes made for keyword/class pairings:',font=(12),p=(0, 0 or (5, 5)))])\n",
    "        for newval in createClass:\n",
    "            layout.append([sg.Text(newval)])\n",
    "        layout.append([sg.Button('SAVE', font=(12))])\n",
    "        win =sg.Window('Customise your Journey',layout)\n",
    "        e,value=win.read()\n",
    "        win.close()\n",
    "\n",
    "    for i in value:\n",
    "        result.update({i:value[i]})\n",
    "    \n",
    "    return tempdictionary\n",
    "\n",
    "def createNewClass(createClass):\n",
    "    # To do: Create New Class\n",
    "    y = list(createClass.values())\n",
    "    SuperClass =  onto.Thing #static value (root class)\n",
    "    for index, val in enumerate(createClass):\n",
    "        with onto:\n",
    "            yvalue = y[index]\n",
    "            NewClass = types.new_class(yvalue, (Thing,))\n",
    "\n",
    "def specifyDomainAndRange(result, createClass):\n",
    "    #To do: Add relationship to class: Specify Domain and Range. Everything should be related to the unique column in the database\n",
    "    combinedDicitonary = {**result, **createClass}\n",
    "    targetdomain = removespaces(tempstring)\n",
    "    # m = re.sub(\"^[^.]*.\",\"\",combinedDicitonary[targetdomain])\n",
    "    targetvalue = combinedDicitonary[targetdomain]\n",
    "    targetdomainclass = onto.search_one(iri=\"*\"+(targetdomain.replace(\".\",\"#\")))\n",
    "    excludecolumn = df.columns[0]\n",
    "\n",
    "    for index, val in enumerate(createClass):\n",
    "        if val is not excludecolumn:  \n",
    "            with onto:\n",
    "                val = removespaces(str(val))\n",
    "                class has(targetdomainclass >> onto[str(val)]): \n",
    "                    pass\n",
    "    for index2, val2 in enumerate(result):\n",
    "        if val2 is not excludecolumn:\n",
    "            rangeclass = onto.search_one(iri=\"*\"+result[val2])\n",
    "            if rangeclass is None:\n",
    "                rangeclass = onto.search_one(is_a=onto[str(re.sub(\"^[^.]*.\",\"\",result[val2]))])\n",
    "                with onto:\n",
    "                    class has(targetdomainclass >> rangeclass): \n",
    "                        pass\n",
    "\n",
    "                    \n",
    "def addInstancesToClasses(result):\n",
    "    #To do: Add instances to class (get classes, match right key word to the class and add instance to it)\n",
    "    objectProperties = list(onto.object_properties())\n",
    "    for index, val in enumerate(result):\n",
    "        temp = re.sub(\"^[^.]*.\",\"\",result[val])\n",
    "        rangeclass = onto.search_one(iri=\"*\"+result[val])\n",
    "        if rangeclass is None:\n",
    "            rangeclass = onto.search_one(is_a=onto[str(re.sub(\"^[^.]*.\",\"\",result[val]))])\n",
    "        for col in df:\n",
    "            if col == val:\n",
    "                for i, row_value in df[col].iteritems():\n",
    "                    if (row_value is not None) and (row_value != 0) and (row_value != 0.0):\n",
    "                        insertvalue = rangeclass(str(row_value))\n",
    "                        insertvalue.label = str(insertvalue)\n",
    "                        if col is not excludecolumn:\n",
    "                            targetdomainclass.instances()[i].has.append(insertvalue)\n",
    "                            for indexObjectProperty in range(len(objectProperties)):\n",
    "                                x = objectProperties[indexObjectProperty].domain\n",
    "                                y = objectProperties[indexObjectProperty].range\n",
    "                                if (x is not None) and (len(x) == 1) and (x[0] == rangeclass):\n",
    "                                    indexPosition.append([x[0],insertvalue,i])\n",
    "\n",
    "def addInstancesToNewClasses(createClass):\n",
    "    # To do: Add instances to the new classes (get classes, match right key word to the class and add instance to it)\n",
    "    if bool(createClass) == True:\n",
    "        for index, val in enumerate(createClass):\n",
    "              for col in df:\n",
    "                if col == val:\n",
    "                    for i, row_value in df[col].iteritems():\n",
    "                        if (row_value is not None) and (row_value != 0) and (row_value != 0.0):\n",
    "                            val = removespaces(val)\n",
    "                            x = onto[str(val)]\n",
    "                            temp =  x(str(row_value))\n",
    "                            temp.label = str(temp)\n",
    "                            if col is not excludecolumn:\n",
    "                                targetdomainclass.instances()[i].has.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc14b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSVtoOntology():\n",
    "    keywordsList = preprocessingCSV()\n",
    "    result, createClass, choices = SBert(onto,keywordsList)\n",
    "    tempdictionary = rightClassMapping(choices)\n",
    "    createNewClass(createClass)\n",
    "    specifyDomainAndRange(result, createClass)\n",
    "    indexPosition = []\n",
    "    if targetdomainclass == list(result.values())[0]:\n",
    "        addInstancesToClasses(result)\n",
    "        addInstancesToNewClasses(createClass)\n",
    "    else:\n",
    "        addInstancesToNewClasses(createClass)\n",
    "        addInstancesToClasses(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db0aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVtoOntology()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45440a6",
   "metadata": {},
   "source": [
    "## Save Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df20c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To do:Save ontology\n",
    "try:\n",
    "    onto.save(file = \"./ontology_template.rdf\", format = \"rdfxml\")\n",
    "except ValueError:\n",
    "    print(ValueError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b4b6c",
   "metadata": {},
   "source": [
    "## Upload to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c985feae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint already existed\n",
      "Sucessfully uploaded XML/RDF/OWL file \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Record terminationStatus='OK' triplesLoaded=2732 triplesParsed=2732 namespaces={'owl': 'http://www.w3.org/2002/07/owl#', 'ns0': 'http://www.semanticweb.org/ontologies/2011/9/Ontology1318785573683.owl#', 'rdfs': 'http://www.w3.org/2000/01/rdf-schema#'} extraInfo='' callParams={'verifyUriSyntax': False}>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Upload ontology to Neo4j for graphing \n",
    "class Neo4j_connection: #neo4jconnection #used to be COnnect2Neo4j\n",
    "\n",
    "    def __init__(self, uri, user, pwd, database_name=\"neo4j\"): #connect to the server; create constraint if not exist; default database is \"Neo4j\"\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None\n",
    "        self.__database_name = database_name\n",
    "\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create driver:\", e)\n",
    "\n",
    "        # check if the neccessary config already existed. if not, create.\n",
    "        query_string = '''\n",
    "        CALL db.constraints()\n",
    "                 '''\n",
    "        a = self.query(query_string, db= self.__database_name)\n",
    "        constr = str(a)\n",
    "        try:\n",
    "            b = constr[constr.find(\"description='\")+13:constr.find(' ON')]\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        if b == 'CONSTRAINT':\n",
    "            print(\"Constraint already existed\")\n",
    "        else:\n",
    "            try:\n",
    "                query_string = '''\n",
    "                    CREATE CONSTRAINT n10s_unique_uri FOR (r:Resource) REQUIRE r.uri IS UNIQUE  \n",
    "                            '''\n",
    "                self.query(query_string, db = self.__database_name)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to create constraint - check again\", e)\n",
    "\n",
    "        # check if the neccessary config already existed. if not, create.\n",
    "        query_string = '''\n",
    "        match(n) return count(n)\n",
    "        '''\n",
    "        a = self.query(query_string, db= self.__database_name)\n",
    "        strin = str(a[0])\n",
    "        config_check = int(strin[strin.find(\"=\")+1:strin.find(\">\")])\n",
    "        if config_check != 0:\n",
    "            print(\"Config already existed\")\n",
    "        else:\n",
    "            try: #setting up neccessary config for neosemantics\n",
    "                query_string = '''\n",
    "                    CALL n10s.graphconfig.init()\n",
    "                                '''\n",
    "                self.query(query_string, db= self.__database_name)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Failed to initiate graphconfig: \", e)\n",
    "\n",
    "\n",
    "    def uploading_orx(self, address, db=\"neo4j\"): #uploading OWL, RDF, and XML #i am trying to\n",
    "        query_string = (\n",
    "                \"call n10s.rdf.import.fetch('\"+address+\"','RDF/XML',\"+\"{\"+\"verifyUriSyntax: false\"+\"}\"+\")\")\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try:\n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session()\n",
    "            response = list(session.run(query_string))\n",
    "            print(\"Sucessfully uploaded XML/RDF/OWL file \")\n",
    "        except Exception as e:\n",
    "            print(\"Upload failed:\", e)\n",
    "        finally:\n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response\n",
    "\n",
    "\n",
    "    def uploading_ttl(self, address, db=None): #uploading Turtle files\n",
    "        query_string = (\n",
    "                \"call n10s.rdf.import.fetch(\" + f'{address}'+  ',\"Turtle\",{verifyUriSyntax: false})')\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try:\n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session()\n",
    "            response = list(session.run(query_string))\n",
    "            print(\"Sucessfully uploaded TTL file \")\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally:\n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "\n",
    "    def query(self, query, db=None): #query commands of choice.\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try:\n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session()\n",
    "            response = list(session.run(query))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally:\n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response\n",
    "\n",
    "a = Neo4j_connection(\"bolt://localhost:7687\", \"neo4j\", \"1234567\")\n",
    "a.uploading_orx(\"file:///Users/jhealynsamson/AP1/ontology_template.rdf\",\"neo4j\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
